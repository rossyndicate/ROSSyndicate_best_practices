# Workflow Management with {targets}

## Using the {targets} package to automate your analyses

Data science workflows will often grow beyond a single script and can rapidly
become unwieldy when many steps, datasets, and files are involved. This is both
a challenge to keep track of (i.e., which scripts to run first, second, etc.)
and a challenge to communicate to other users in the world of open science.

Workflow management software is a solution to this that the ROSSyndicate has
embraced. Specifically, we use the
[{targets}](https://books.ropensci.org/targets/) R package. {targets} in
particular allows you to use R to build out a pipeline (used interchangeably
with workflow here) which the software automatically tracks so that it "knows"
which steps have been run, which have failed, and which need to be re-run. Your
entire analytical pipeline or workflow can be rerun with just the command
`tar_make()` and you can visualize the connections between objects and
functions in your pipeline with `tar_visnetwork()`.

One standout strength of this workflow management architecture is that the
software "knows" which downstream steps are affected by a change in the
upstream code or datasets. This means that you as a human have no need to
manually re-run scripts that are outdated by changes in code; {targets} knows
which are affected, can tell you, and automatically re-run them.

Not every analysis is a perfect fit for a {targets} workflow, however. Very
small analyses may not be worth the time needed to invest to build out the
structure of the pipeline. Workflows that are often re-run with updated
datasets or which can perform many parallel iterations are especially good
candidates for the {targets} pipeline style. Because the ROSSyndicate values
literate code and {targets} does not work with Markdown or Quarto documents,
there are additional steps to creating easilyt understandable and literate code
within the {targets} framework.

We won't explain how to use {targets} in detail here, but good resources are
readily available. [The {targets} R package user
manual](https://books.ropensci.org/targets/) is a great resource written by the
author of the {targets} package. Matt Brousil from the ROSSyndicate has also
written a short example of building an analysis with the package in [Targets
for Ecologists](https://targets-ecology.netlify.app/) and created an
[instructional video](https://www.youtube.com/watch?v=UG4xqYcDpxw).


### Hot tips and tricks

#### Sourcing scripts for functions in {targets} pipelines

The `_targets.R` script is an essential piece of a {targets} pipeline. This
file is also where functions that define workflow steps ("targets") are sourced
from other scripts. {targets} provides a built-in function to do this,
`tar_source()`. For example: `tar_source("src/custom_functions.R")`


#### Package use in a {targets} pipeline

You might find it unclear how to require certain packages when writing code for
a {targets} workflow. The ideal situation is that you use the
`tar_option_set()` function near the top of your `_targets.R` script and
provide the `packages` argument a character vector of package names that should
be loaded for ***every*** target in the workflow. This often might just be
`tar_option_set(packages = c("tidyverse"))`. Then, you can change this default
for specific targets within the respective targets's `tar_target()` function
like this: `tar_target(packages = c("tidyverse", "lubridate"))`. Note that
you'll need to still repeat any packages you've already listed in
`tar_option_set()` when changing the packages argument in `tar_target()`.


#### Reading external files into {targets} pipelines

When building your {targets} pipeline you may wonder how best to read in data
and have the pipeline track the existence of input files. The
[{tarchetypes}](https://docs.ropensci.org/tarchetypes/) package provides some
handy shortcut functions for this: `tar_file()` and `tar_file_read()`. The
first, `tar_file()`, identifies that a pipeline target is a dynamic file based
on a path provided to the `command` argument. By contrast, `tar_file_read()`
will create two pipeline targets: one tracking the file path provided, and a
second reading in the file given the instructions you provide to the `read`
argument.

For instance, if you have a .yml file that you want to track for changes, you
can use the `tar_file()` method for tracking. If you have a file (for example a
.csv) that is output from a target that you want to use later, your target that
creates that .csv would return the _filepath_ of that .csv. You could then use
`tar_read()` to store the data in that .csv as a target and track the file (as
your _filepath_).

#### Using the 'branching' function in {targets}

There are times where your workflow will benefit from tracking specific inputs
into a target. For instance, in the Landsat acquisition workflow, we repeat a
function over every single WRS path-row over the United States. Sometimes this
acquisition fails because it takes a **long time** to execute. Deploying
branches requires two steps: first, you create a list to iterate (or branch) on
- in this case, a list of path-rows; then you map that list over the function
that relies on the path-row. Your {targets} list might look something like
this:


```{r, eval = FALSE}
list(
  # get WRS tiles - this target returns a list of path-rows
  tar_target(
    name = WRS_tiles,
    command = get_WRS_tiles(WRS_detection_method, yml_file),
    packages = c("readr", "sf")
  ),

  # run the landsat pull as function per path-row
  tar_target(
    name = eeRun,
    command = {
      ref_pull_457_DSWE1
      ref_pull_89_DSWE1
      run_GEE_per_tile(WRS_tiles)
    },
    pattern = map(WRS_tiles), # this is where the magic happens!
    packages = "reticulate"
  )
)
```

#### Using python scripts in {targets}

The {targets} package was designed to work for R workflows only, which means
for our mixed-language workflows, we have to get creative. Again, {targets}
will not render AND track the output of a .Rmd file, which means all of our
precious .Rmd must be translated into .R and .py scripts in order to work in
{targets}. We employ the use of `source_python()` from the {reticulate}
package. You can use this in the same way you might use `tar_source()` to track
all of the functions you call in your pipeline. {targets} will track all of the
functions in your .py file as individual targets. To run a python script as a
target, you have to nest it in an R function. For instance, our
`run_GEE_per_tile()` from above is just this wrapper to:

```{r, eval = F}
run_GEE_per_tile <- function(WRS_tile) {
  tile <- WRS_tile # store the tile from the list
  write_lines(tile, 'data_acquisition/out/current_tile.txt', sep = '') # save it as a generic file
  source_python('data_acquisition/src/runGEEperTile.py') # this script actually *reads* the generic file above as one of the first steps
}
```

Note, {targets} does not track any of the functions called in the
`source_python()` line like it does in an .R script - you must list all of the
functions that are needed in your script in the `command` list so that
{targets} knows you need those functions to run the code. Additionally, targets
does not actively track any output from `source_python()`, so again, you'll
have to get creative!

### Resources

Below are links to resources to help you learn more about {targets} and
implementing workflow management software:

+ [The {targets} R package user manual](https://books.ropensci.org/targets/) +
[Improving ecological data science with workflow management
software](https://doi.org/10.1111/2041-210X.14113) + [Targets for
Ecologists](https://targets-ecology.netlify.app/)


```{r, echo = F}
knitr::wrap_rmd('09-Workflow-Management-with-targets.Rmd', width = 80, backup = NULL)
#note, this will not wrap text that are prefaced by any special characters (like bullets!)
```
